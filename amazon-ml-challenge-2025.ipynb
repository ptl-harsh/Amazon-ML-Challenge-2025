{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:04.527666Z",
     "iopub.status.busy": "2025-10-12T10:43:04.527458Z",
     "iopub.status.idle": "2025-10-12T10:43:05.588716Z",
     "shell.execute_reply": "2025-10-12T10:43:05.587911Z",
     "shell.execute_reply.started": "2025-10-12T10:43:04.527646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Smart Product Pricing \n",
    "# (TF-IDF -> SVD, Target Encodings, LGB/XGB + Linear Stack, Stacker, Bias Fix)\n",
    "# ============================================================\n",
    "\n",
    "import os, re, gc, time, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "T0 = time.time()\n",
    "def tic(msg): \n",
    "    print(f\"\\n[START] {msg}\")\n",
    "    return time.time()\n",
    "def toc(t0, msg): \n",
    "    print(f\"[END] {msg} -> {time.time()-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:09.097271Z",
     "iopub.status.busy": "2025-10-12T10:43:09.096673Z",
     "iopub.status.idle": "2025-10-12T10:43:09.101393Z",
     "shell.execute_reply": "2025-10-12T10:43:09.100721Z",
     "shell.execute_reply.started": "2025-10-12T10:43:09.097247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------- Tunables (safe defaults) ----------------\n",
    "SEED        = 42\n",
    "N_SPLITS    = 5            # 3 if memory tight\n",
    "N_COMP      = 448          # 384 or 256 if memory tight\n",
    "TE_ALPHA    = 15.0         # target encoding smoothing\n",
    "TE_MIN_CNT  = 5            # min category count before shrink\n",
    "LGB_EST     = 2000         # upper bound; early stop will cut\n",
    "LGB_ESR     = 160\n",
    "XGB_ROUNDS  = 2400\n",
    "XGB_ESR     = 180\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:11.649407Z",
     "iopub.status.busy": "2025-10-12T10:43:11.648731Z",
     "iopub.status.idle": "2025-10-12T10:43:11.656209Z",
     "shell.execute_reply": "2025-10-12T10:43:11.655551Z",
     "shell.execute_reply.started": "2025-10-12T10:43:11.649382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR=/kaggle/input/challenge-ml-data\n",
      "WORK_DIR=/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Paths ----------------\n",
    "IS_KAGGLE = Path(\"/kaggle\").exists()\n",
    "WORK_DIR  = Path(\"/kaggle/working\") if IS_KAGGLE else Path(\"/content\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR  = Path(\"/kaggle/input/challenge-ml-data\") if IS_KAGGLE else Path(\"dataset\")\n",
    "TRAIN_CSV = DATA_DIR / \"train.csv\"; TEST_CSV = DATA_DIR / \"test.csv\"\n",
    "print(f\"DATA_DIR={DATA_DIR.resolve()}\\nWORK_DIR={WORK_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:14.729056Z",
     "iopub.status.busy": "2025-10-12T10:43:14.728296Z",
     "iopub.status.idle": "2025-10-12T10:43:14.740342Z",
     "shell.execute_reply": "2025-10-12T10:43:14.739618Z",
     "shell.execute_reply.started": "2025-10-12T10:43:14.729030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Utils ----------------\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    m = denom != 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[m] = np.abs(y_true[m] - y_pred[m]) / denom[m]\n",
    "    return np.mean(out) * 100.0\n",
    "\n",
    "def inv(z):\n",
    "    return np.expm1(np.clip(z, -50, 50))\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "    s = re.sub(r\"\\s+\",\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_numeric_value_unit(s: str) -> Tuple[float, str]:\n",
    "    if not isinstance(s, str): return 0.0, \"\"\n",
    "    val = 0.0; unit = \"\"\n",
    "    mv = re.search(r\"Value:\\s*([0-9]*\\.?[0-9]+)\", s, flags=re.IGNORECASE)\n",
    "    mu = re.search(r\"Unit:\\s*([A-Za-z ]+)\", s, flags=re.IGNORECASE)\n",
    "    if mv:\n",
    "        try: val = float(mv.group(1))\n",
    "        except: val = 0.0\n",
    "    if mu: unit = mu.group(1).strip()\n",
    "    return val, unit\n",
    "\n",
    "def standardize_quantity(value: float, unit: str) -> Tuple[float, float, float]:\n",
    "    if unit is None: unit = \"\"\n",
    "    u = unit.strip().lower()\n",
    "    v = float(value) if value is not None else 0.0\n",
    "    if \"count\" in u or u == \"ct\" or \"pcs\" in u or \"piece\" in u: return 0.0, 0.0, v\n",
    "    if \"pound\" in u or \"lb\" in u: return v*453.592, 0.0, 0.0\n",
    "    if \"ounce\" in u or u == \"oz\":\n",
    "        if \"fl\" in u: return 0.0, v*29.5735, 0.0\n",
    "        return v*28.3495, 0.0, 0.0\n",
    "    if \"gram\" in u or u == \"g\": return v, 0.0, 0.0\n",
    "    if \"kg\" in u or \"kilogram\" in u: return v*1000.0, 0.0, 0.0\n",
    "    if \"ml\" in u: return 0.0, v, 0.0\n",
    "    if u == \"l\" or \"liter\" in u or \"litre\" in u: return 0.0, v*1000.0, 0.0\n",
    "    if \"fl oz\" in u or \"fluid ounce\" in u or \"fl\" in u: return 0.0, v*29.5735, 0.0\n",
    "    return 0.0, 0.0, 0.0\n",
    "\n",
    "def parse_pack_count(s: str) -> float:\n",
    "    if not isinstance(s, str): return 1.0\n",
    "    m = re.search(r\"(?:pack of|case of)\\s*([0-9]+)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        try: return float(m.group(1))\n",
    "        except: return 1.0\n",
    "    return 1.0\n",
    "\n",
    "def parse_title_itemname(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    m = re.search(r\"Item Name:\\s*(.+?)(?:\\s*Bullet Point|\\s*Product Description:|\\s*Value:|\\s*$)\",\n",
    "                  s, flags=re.IGNORECASE|re.DOTALL)\n",
    "    return clean_text(m.group(1)) if m else \"\"\n",
    "\n",
    "def maybe_brand_from_title(title: str) -> str:\n",
    "    if not isinstance(title, str) or not title: return \"\"\n",
    "    toks = re.split(r\"[|,\\-\\(\\)\\/\\s]+\", title)\n",
    "    return toks[0][:40] if toks else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:18.690320Z",
     "iopub.status.busy": "2025-10-12T10:43:18.689610Z",
     "iopub.status.idle": "2025-10-12T10:43:47.362939Z",
     "shell.execute_reply": "2025-10-12T10:43:47.362150Z",
     "shell.execute_reply.started": "2025-10-12T10:43:18.690296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Load & basic features\n",
      "Shapes: (75000, 4) (75000, 3)\n",
      "[END] Load & basic features -> 28.65s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Load & basic features ----------------\n",
    "t0 = tic(\"Load & basic features\")\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "print(\"Shapes:\", train.shape, test.shape)\n",
    "assert {\"sample_id\",\"catalog_content\",\"image_link\",\"price\"}.issubset(train.columns)\n",
    "assert {\"sample_id\",\"catalog_content\",\"image_link\"}.issubset(test.columns)\n",
    "\n",
    "for df in (train, test):\n",
    "    df[\"catalog_content\"] = df[\"catalog_content\"].astype(str).apply(clean_text)\n",
    "    df[\"title\"] = df[\"catalog_content\"].apply(parse_title_itemname)\n",
    "    df[\"brand_heur\"] = df[\"title\"].apply(maybe_brand_from_title)\n",
    "\n",
    "    v_u = df[\"catalog_content\"].apply(extract_numeric_value_unit)\n",
    "    df[\"value_extracted\"] = v_u.apply(lambda x: float(x[0]) if x and x[0] is not None else 0.0)\n",
    "    df[\"unit_extracted\"]  = v_u.apply(lambda x: x[1] if x and x[1] is not None else \"\")\n",
    "\n",
    "    q_std = df.apply(lambda r: standardize_quantity(r[\"value_extracted\"], r[\"unit_extracted\"]),\n",
    "                     axis=1, result_type=\"expand\")\n",
    "    df[\"qty_g\"]  = q_std[0].astype(float)\n",
    "    df[\"qty_ml\"] = q_std[1].astype(float)\n",
    "    df[\"qty_cnt\"]= q_std[2].astype(float)\n",
    "\n",
    "    df[\"pack_count\"] = df[\"catalog_content\"].apply(parse_pack_count).astype(float)\n",
    "    df[\"len_chars\"] = df[\"catalog_content\"].str.len()\n",
    "    df[\"len_words\"] = df[\"catalog_content\"].apply(lambda s: len(s.split()))\n",
    "    df[\"upper_ratio\"] = df[\"catalog_content\"].apply(lambda s: (sum(1 for ch in s if ch.isupper()) / (len(s)+1e-6)))\n",
    "toc(t0, \"Load & basic features\")\n",
    "\n",
    "# Targets\n",
    "y = train[\"price\"].astype(float).values\n",
    "y_log = np.log1p(np.clip(y, 0, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:43:56.263649Z",
     "iopub.status.busy": "2025-10-12T10:43:56.263332Z",
     "iopub.status.idle": "2025-10-12T10:44:03.304056Z",
     "shell.execute_reply": "2025-10-12T10:44:03.303417Z",
     "shell.execute_reply.started": "2025-10-12T10:43:56.263629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Enhanced quantity parsing\n",
      "[END] Enhanced quantity parsing -> 7.03s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Enhanced quantity parsing ----------------\n",
    "t0 = tic(\"Enhanced quantity parsing\")\n",
    "_q_rgx_multi = re.compile(\n",
    "    r'(\\d+)\\s*[xX×]\\s*([0-9]*\\.?[0-9]+)\\s*(kg|g|lb|oz|ml|l|litre|liter|fl\\s*oz)\\b',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "_unit_map_w = {\"kg\":1000.0, \"g\":1.0, \"lb\":453.592, \"oz\":28.3495}\n",
    "_unit_map_v = {\"l\":1000.0, \"litre\":1000.0, \"liter\":1000.0, \"ml\":1.0, \"fl oz\":29.5735}\n",
    "def parse_multi_qty(s: str):\n",
    "    if not isinstance(s, str): return (0.0, 0.0, 1.0)\n",
    "    s_ = s.lower().replace(\"fluid ounce\",\"fl oz\").replace(\"fl. oz\",\"fl oz\")\n",
    "    m = _q_rgx_multi.search(s_)\n",
    "    w_g = 0.0; v_ml = 0.0; cnt = parse_pack_count(s)\n",
    "    if m:\n",
    "        n = float(m.group(1)); val = float(m.group(2)); u = m.group(3).strip().lower()\n",
    "        if u in _unit_map_w: w_g = n * val * _unit_map_w[u]\n",
    "        elif u in _unit_map_v: v_ml = n * val * _unit_map_v[u]\n",
    "    return (w_g, v_ml, cnt)\n",
    "\n",
    "for df in (train, test):\n",
    "    df[\"qty_g2\"], df[\"qty_ml2\"], df[\"pack_cnt2\"] = zip(*df[\"catalog_content\"].map(parse_multi_qty))\n",
    "    df[\"qty_g_tot\"]  = (df[\"qty_g\"].fillna(0)  + df[\"qty_g2\"].fillna(0)).astype(float)\n",
    "    df[\"qty_ml_tot\"] = (df[\"qty_ml\"].fillna(0) + df[\"qty_ml2\"].fillna(0)).astype(float)\n",
    "    df[\"pack_cnt_tot\"] = np.maximum(df[\"pack_count\"].fillna(1.0), df[\"pack_cnt2\"].fillna(1.0)).astype(float)\n",
    "    df[\"per_unit_g\"]  = (df[\"qty_g_tot\"]  / (df[\"pack_cnt_tot\"]+1e-6)).clip(0, 5e6)\n",
    "    df[\"per_unit_ml\"] = (df[\"qty_ml_tot\"] / (df[\"pack_cnt_tot\"]+1e-6)).clip(0, 5e6)\n",
    "toc(t0, \"Enhanced quantity parsing\")\n",
    "\n",
    "# numeric columns (optimized)\n",
    "num_cols = [\n",
    "    \"qty_g_tot\",\"qty_ml_tot\",\"pack_cnt_tot\",\"per_unit_g\",\"per_unit_ml\",\n",
    "    \"len_chars\",\"len_words\",\"upper_ratio\",\"value_extracted\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:44:09.051928Z",
     "iopub.status.busy": "2025-10-12T10:44:09.051357Z",
     "iopub.status.idle": "2025-10-12T10:50:28.411525Z",
     "shell.execute_reply": "2025-10-12T10:50:28.410747Z",
     "shell.execute_reply.started": "2025-10-12T10:44:09.051903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] TF-IDF fit/transform\n",
      "[END] TF-IDF fit/transform -> 379.35s\n"
     ]
    }
   ],
   "source": [
    "# ---------------- TF-IDF (word+char, stronger) ----------------\n",
    "t0 = tic(\"TF-IDF fit/transform\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "text_tr = (train[\"title\"].fillna(\"\") + \" \" + train[\"catalog_content\"].fillna(\"\") + \" \" + train[\"brand_heur\"].fillna(\"\")).tolist()\n",
    "text_te = (test[\"title\"].fillna(\"\")  + \" \" + test[\"catalog_content\"].fillna(\"\")  + \" \" + test[\"brand_heur\"].fillna(\"\")).tolist()\n",
    "\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    min_df=4,            # a touch lower to keep useful rare tokens\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1,2),\n",
    "    strip_accents=\"unicode\",\n",
    "    sublinear_tf=True,   # helps long docs\n",
    "    norm=\"l2\"\n",
    ")\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer=\"char\",\n",
    "    ngram_range=(3,6),   # capture more unit/pattern variants\n",
    "    min_df=4,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "Xw_tr = tfidf_word.fit_transform(text_tr)\n",
    "Xw_te = tfidf_word.transform(text_te)\n",
    "Xc_tr = tfidf_char.fit_transform(text_tr)\n",
    "Xc_te = tfidf_char.transform(text_te)\n",
    "\n",
    "Xtxt_tr = sparse.hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "Xtxt_te = sparse.hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "del Xw_tr, Xw_te, Xc_tr, Xc_te; gc.collect()\n",
    "toc(t0, \"TF-IDF fit/transform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:50:59.896214Z",
     "iopub.status.busy": "2025-10-12T10:50:59.895470Z",
     "iopub.status.idle": "2025-10-12T10:51:15.670106Z",
     "shell.execute_reply": "2025-10-12T10:51:15.669239Z",
     "shell.execute_reply.started": "2025-10-12T10:50:59.896187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Target encodings + interactions\n",
      "[END] Target encodings + interactions -> 15.76s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Target Encodings (Stratified CV + interactions) ----------------\n",
    "t0 = tic(\"Target encodings + interactions\")\n",
    "def first_two_tokens(s):\n",
    "    if not isinstance(s, str): return (\"\",\"\")\n",
    "    t = s.split()\n",
    "    a = t[0] if len(t) > 0 else \"\"\n",
    "    b = t[1] if len(t) > 1 else \"\"\n",
    "    return (a, b)\n",
    "train[[\"tok1\",\"tok2\"]] = train[\"title\"].fillna(\"\").apply(first_two_tokens).apply(pd.Series)\n",
    "test[[\"tok1\",\"tok2\"]]  = test[\"title\"].fillna(\"\").apply(first_two_tokens).apply(pd.Series)\n",
    "\n",
    "train[\"brand_unit\"] = (train[\"brand_heur\"].fillna(\"\") + \"§\" + train[\"unit_extracted\"].fillna(\"\")).astype(str)\n",
    "test[\"brand_unit\"]  = (test[\"brand_heur\"].fillna(\"\")  + \"§\" + test[\"unit_extracted\"].fillna(\"\")).astype(str)\n",
    "train[\"tok1_unit\"]  = (train[\"tok1\"].fillna(\"\")       + \"§\" + train[\"unit_extracted\"].fillna(\"\")).astype(str)\n",
    "test[\"tok1_unit\"]   = (test[\"tok1\"].fillna(\"\")        + \"§\" + test[\"unit_extracted\"].fillna(\"\")).astype(str)\n",
    "\n",
    "enc_cols = [\"brand_heur\",\"tok1\",\"tok2\",\"unit_extracted\",\"brand_unit\",\"tok1_unit\"]\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "bins = pd.qcut(y_log, q=10, duplicates=\"drop\").codes\n",
    "SKF = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "def cv_target_encode_strat(df_tr, df_te, y_log, col, alpha=TE_ALPHA, min_count=TE_MIN_CNT):\n",
    "    oof = np.zeros(len(df_tr), dtype=np.float32)\n",
    "    global_mean = float(np.mean(y_log))\n",
    "    for tr_idx, va_idx in SKF.split(df_tr, bins):\n",
    "        tr_g = df_tr.iloc[tr_idx][col]; va_g = df_tr.iloc[va_idx][col]; tr_y = y_log[tr_idx]\n",
    "        s = pd.DataFrame({\"g\": tr_g.values, \"y\": tr_y})\n",
    "        agg = s.groupby(\"g\")[\"y\"].agg([\"sum\",\"count\"]).reset_index()\n",
    "        agg[\"mean_smooth\"] = (agg[\"sum\"] + alpha*global_mean) / (agg[\"count\"] + alpha)\n",
    "        enc = dict(zip(agg[\"g\"], agg[\"mean_smooth\"]))\n",
    "        oof[va_idx] = va_g.map(enc).fillna(global_mean).values.astype(np.float32)\n",
    "    sfull = pd.DataFrame({\"g\": df_tr[col].values, \"y\": y_log})\n",
    "    agg = sfull.groupby(\"g\")[\"y\"].agg([\"sum\",\"count\"]).reset_index()\n",
    "    agg[\"mean_smooth\"] = (agg[\"sum\"] + alpha*global_mean) / (agg[\"count\"] + alpha)\n",
    "    enc_full = dict(zip(agg[\"g\"], agg[\"mean_smooth\"]))\n",
    "    te = df_te[col].map(enc_full).fillna(global_mean).values.astype(np.float32)\n",
    "    return oof, te\n",
    "\n",
    "enc_tr_list, enc_te_list = [], []\n",
    "for c in enc_cols:\n",
    "    oof_c, te_c = cv_target_encode_strat(train, test, y_log, c)\n",
    "    train[f\"enc_{c}\"] = oof_c; test[f\"enc_{c}\"] = te_c\n",
    "    enc_tr_list.append(oof_c); enc_te_list.append(te_c)\n",
    "\n",
    "enc_tr = np.vstack(enc_tr_list).T.astype(np.float32)\n",
    "enc_te = np.vstack(enc_te_list).T.astype(np.float32)\n",
    "toc(t0, \"Target encodings + interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T10:52:48.640004Z",
     "iopub.status.busy": "2025-10-12T10:52:48.639366Z",
     "iopub.status.idle": "2025-10-12T11:15:37.290648Z",
     "shell.execute_reply": "2025-10-12T11:15:37.289990Z",
     "shell.execute_reply.started": "2025-10-12T10:52:48.639981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] SVD(448) + compact matrices\n",
      "SVD explained variance ~ 0.247\n",
      "Compact shapes: (75000, 463) (75000, 463)\n",
      "[END] SVD + compact -> 1368.65s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- SVD compact features ----------------\n",
    "t0 = tic(f\"SVD({N_COMP}) + compact matrices\")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=N_COMP, random_state=SEED)\n",
    "Ztr_txt = svd.fit_transform(Xtxt_tr).astype(np.float32)\n",
    "Zte_txt = svd.transform(Xtxt_te).astype(np.float32)\n",
    "print(f\"SVD explained variance ~ {svd.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "N_tr_num = train[num_cols].astype(np.float32).values\n",
    "N_te_num = test[num_cols].astype(np.float32).values\n",
    "\n",
    "Xr_tr = np.hstack([Ztr_txt, N_tr_num, enc_tr]).astype(np.float32)\n",
    "Xr_te = np.hstack([Zte_txt, N_te_num, enc_te]).astype(np.float32)\n",
    "print(\"Compact shapes:\", Xr_tr.shape, Xr_te.shape)\n",
    "\n",
    "del Ztr_txt, Zte_txt, N_tr_num, N_te_num, enc_tr, enc_te, Xtxt_tr, Xtxt_te\n",
    "gc.collect()\n",
    "toc(t0, \"SVD + compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T11:15:43.437017Z",
     "iopub.status.busy": "2025-10-12T11:15:43.436419Z",
     "iopub.status.idle": "2025-10-12T14:39:13.162430Z",
     "shell.execute_reply": "2025-10-12T14:39:13.161754Z",
     "shell.execute_reply.started": "2025-10-12T11:15:43.436992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Models (Linear + LGB-GBDT + LGB-DART + XGB)\n",
      "\n",
      "===== Stratified 5-Fold: Ridge + ENet + Huber =====\n",
      "Fold 1  R=70.658  E=58.734  H=68.602\n",
      "Fold 2  R=70.108  E=58.026  H=68.919\n",
      "Fold 3  R=70.222  E=58.153  H=64.393\n",
      "Fold 4  R=70.009  E=58.218  H=64.031\n",
      "Fold 5  R=70.193  E=57.907  H=68.178\n",
      "OOF  Ridge: 70.238\n",
      "OOF  ENet : 58.208\n",
      "OOF  Huber: 66.825\n",
      "\n",
      "===== Stratified 5-Fold: LightGBM-GBDT (compact) =====\n",
      "Fold 1  LGB-GBDT SMAPE=49.092\n",
      "Fold 2  LGB-GBDT SMAPE=48.884\n",
      "Fold 3  LGB-GBDT SMAPE=49.228\n",
      "Fold 4  LGB-GBDT SMAPE=48.990\n",
      "Fold 5  LGB-GBDT SMAPE=48.609\n",
      "OOF  LGB-GBDT : 48.961\n",
      "\n",
      "===== Stratified 5-Fold: LightGBM-DART (compact) =====\n",
      "Fold 1  LGB-DART SMAPE=195.760\n",
      "Fold 2  LGB-DART SMAPE=195.838\n",
      "Fold 3  LGB-DART SMAPE=195.839\n",
      "Fold 4  LGB-DART SMAPE=195.832\n",
      "Fold 5  LGB-DART SMAPE=195.970\n",
      "OOF  LGB-DART : 195.848\n",
      "\n",
      "===== Stratified 5-Fold: XGBoost (compact, tighter) =====\n",
      "Fold 1  XGB SMAPE=50.134\n",
      "Fold 2  XGB SMAPE=49.894\n",
      "Fold 3  XGB SMAPE=50.148\n",
      "Fold 4  XGB SMAPE=50.018\n",
      "Fold 5  XGB SMAPE=49.502\n",
      "OOF  XGB : 49.939\n",
      "\n",
      "Models available: ['Ridge', 'ENet', 'Huber', 'LGB-GBDT', 'LGB-DART', 'XGB']\n",
      "[END] Models (Linear + LGB-GBDT + LGB-DART + XGB) -> 12209.71s\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Models: Linear + LGB(GBDT+DART) + XGB ----------------\n",
    "t0 = tic(\"Models (Linear + LGB-GBDT + LGB-DART + XGB)\")\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor\n",
    "\n",
    "oofs, preds, names = [], [], []\n",
    "\n",
    "# Linear trio\n",
    "oof_r = np.zeros(len(train), np.float32); pr_r = np.zeros(len(test), np.float32)\n",
    "oof_e = np.zeros(len(train), np.float32); pr_e = np.zeros(len(test), np.float32)\n",
    "oof_h = np.zeros(len(train), np.float32); pr_h = np.zeros(len(test), np.float32)\n",
    "\n",
    "print(\"\\n===== Stratified 5-Fold: Ridge + ENet + Huber =====\")\n",
    "for f, (tr_idx, va_idx) in enumerate(SKF.split(Xr_tr, bins), 1):\n",
    "    Xtr, Xva = Xr_tr[tr_idx], Xr_tr[va_idx]; ytr, yva = y_log[tr_idx], y_log[va_idx]\n",
    "    r = Ridge(alpha=1.8, solver=\"lsqr\", max_iter=20000, tol=1e-4).fit(Xtr, ytr)\n",
    "    e = ElasticNet(alpha=0.001, l1_ratio=0.12, max_iter=4500, tol=1e-3).fit(Xtr, ytr)\n",
    "    h = HuberRegressor(alpha=1e-4, epsilon=1.25, max_iter=220).fit(Xtr, ytr)\n",
    "    pv_r, pt_r = r.predict(Xva), r.predict(Xr_te)\n",
    "    pv_e, pt_e = e.predict(Xva), e.predict(Xr_te)\n",
    "    pv_h, pt_h = h.predict(Xva), h.predict(Xr_te)\n",
    "    oof_r[va_idx], oof_e[va_idx], oof_h[va_idx] = pv_r, pv_e, pv_h\n",
    "    pr_r += pt_r/SKF.n_splits; pr_e += pt_e/SKF.n_splits; pr_h += pt_h/SKF.n_splits\n",
    "    print(f\"Fold {f}  R={smape(inv(yva),inv(pv_r)):.3f}  E={smape(inv(yva),inv(pv_e)):.3f}  H={smape(inv(yva),inv(pv_h)):.3f}\")\n",
    "    del Xtr, Xva; gc.collect()\n",
    "print(\"OOF  Ridge:\", f\"{smape(y, inv(oof_r)):.3f}\")\n",
    "print(\"OOF  ENet :\", f\"{smape(y, inv(oof_e)):.3f}\")\n",
    "print(\"OOF  Huber:\", f\"{smape(y, inv(oof_h)):.3f}\")\n",
    "\n",
    "oofs += [oof_r, oof_e, oof_h]; preds += [pr_r, pr_e, pr_h]; names += [\"Ridge\",\"ENet\",\"Huber\"]\n",
    "\n",
    "# LightGBM (GBDT)\n",
    "HAVE_LGB = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAVE_LGB = True\n",
    "except Exception as e:\n",
    "    print(\"LightGBM unavailable:\", e)\n",
    "\n",
    "if HAVE_LGB:\n",
    "    print(\"\\n===== Stratified 5-Fold: LightGBM-GBDT (compact) =====\")\n",
    "    oof_l = np.zeros(len(train), np.float32); pr_l = np.zeros(len(test), np.float32)\n",
    "    lgb_params_gbdt = dict(\n",
    "        objective='mae', boosting_type='gbdt',\n",
    "        learning_rate=0.03, num_leaves=208, max_depth=-1,\n",
    "        feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
    "        min_data_in_leaf=24, lambda_l1=0.0, lambda_l2=1.2,\n",
    "        max_bin=255,\n",
    "        n_estimators=2200, random_state=SEED, n_jobs=-1, verbose=-1\n",
    "    )\n",
    "    for f, (tr_idx, va_idx) in enumerate(SKF.split(Xr_tr, bins), 1):\n",
    "        Xtr, Xva = Xr_tr[tr_idx], Xr_tr[va_idx]; ytr, yva = y_log[tr_idx], y_log[va_idx]\n",
    "        m = lgb.LGBMRegressor(**lgb_params_gbdt)\n",
    "        m.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='mae',\n",
    "              callbacks=[lgb.early_stopping(stopping_rounds=180, verbose=False)])\n",
    "        pv = m.predict(Xva, num_iteration=m.best_iteration_)\n",
    "        pt = m.predict(Xr_te, num_iteration=m.best_iteration_)\n",
    "        oof_l[va_idx] = pv.astype(np.float32); pr_l += pt.astype(np.float32)/SKF.n_splits\n",
    "        print(f\"Fold {f}  LGB-GBDT SMAPE={smape(inv(yva), inv(pv)):.3f}\")\n",
    "        del Xtr, Xva; gc.collect()\n",
    "    print(\"OOF  LGB-GBDT :\", f\"{smape(y, inv(oof_l)):.3f}\")\n",
    "    oofs.append(oof_l); preds.append(pr_l); names.append(\"LGB-GBDT\")\n",
    "\n",
    "    # LightGBM (DART) — adds diversity; often blends well with GBDT\n",
    "    print(\"\\n===== Stratified 5-Fold: LightGBM-DART (compact) =====\")\n",
    "    oof_ld = np.zeros(len(train), np.float32); pr_ld = np.zeros(len(test), np.float32)\n",
    "    lgb_params_dart = dict(\n",
    "        objective='mae', boosting_type='dart',\n",
    "        learning_rate=0.035, num_leaves=160, max_depth=-1,\n",
    "        feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
    "        min_data_in_leaf=20, lambda_l1=0.0, lambda_l2=1.0,\n",
    "        max_bin=255,\n",
    "        drop_rate=0.1, skip_drop=0.5, uniform_drop=True,\n",
    "        n_estimators=2600, random_state=SEED, n_jobs=-1, verbose=-1\n",
    "    )\n",
    "    for f, (tr_idx, va_idx) in enumerate(SKF.split(Xr_tr, bins), 1):\n",
    "        Xtr, Xva = Xr_tr[tr_idx], Xr_tr[va_idx]; ytr, yva = y_log[tr_idx], y_log[va_idx]\n",
    "        m = lgb.LGBMRegressor(**lgb_params_dart)\n",
    "        m.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='mae',\n",
    "              callbacks=[lgb.early_stopping(stopping_rounds=220, verbose=False)])\n",
    "        pv = m.predict(Xva, num_iteration=m.best_iteration_)\n",
    "        pt = m.predict(Xr_te, num_iteration=m.best_iteration_)\n",
    "        oof_ld[va_idx] = pv.astype(np.float32); pr_ld += pt.astype(np.float32)/SKF.n_splits\n",
    "        print(f\"Fold {f}  LGB-DART SMAPE={smape(inv(yva), inv(pv)):.3f}\")\n",
    "        del Xtr, Xva; gc.collect()\n",
    "    print(\"OOF  LGB-DART :\", f\"{smape(y, inv(oof_ld)):.3f}\")\n",
    "    oofs.append(oof_ld); preds.append(pr_ld); names.append(\"LGB-DART\")\n",
    "\n",
    "# XGBoost\n",
    "HAVE_XGB = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAVE_XGB = True\n",
    "except Exception as e:\n",
    "    print(\"XGBoost unavailable:\", e)\n",
    "\n",
    "if HAVE_XGB:\n",
    "    print(\"\\n===== Stratified 5-Fold: XGBoost (compact, tighter) =====\")\n",
    "    oof_x = np.zeros(len(train), np.float32); pr_x = np.zeros(len(test), np.float32)\n",
    "    params = dict(\n",
    "        objective='reg:absoluteerror',\n",
    "        tree_method='gpu_hist' if os.environ.get(\"CUDA_VISIBLE_DEVICES\") else 'hist',\n",
    "        max_depth=8, min_child_weight=5,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        learning_rate=0.035, reg_alpha=0.0, reg_lambda=1.2,\n",
    "        gamma=0.0, random_state=SEED, verbosity=0\n",
    "    )\n",
    "    for f, (tr_idx, va_idx) in enumerate(SKF.split(Xr_tr, bins), 1):\n",
    "        Xtr, Xva = Xr_tr[tr_idx], Xr_tr[va_idx]; ytr, yva = y_log[tr_idx], y_log[va_idx]\n",
    "        dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xr_te)\n",
    "        try:\n",
    "            m = xgb.train(params=params, dtrain=dtr, num_boost_round=2600,\n",
    "                          evals=[(dtr,\"train\"),(dva,\"valid\")], early_stopping_rounds=200, verbose_eval=False)\n",
    "        except Exception:\n",
    "            params_f = dict(params, max_depth=7)\n",
    "            m = xgb.train(params=params_f, dtrain=dtr, num_boost_round=1800,\n",
    "                          evals=[(dtr,\"train\"),(dva,\"valid\")], early_stopping_rounds=140, verbose_eval=False)\n",
    "        pv = m.predict(dva, iteration_range=(0, m.best_iteration+1))\n",
    "        pt = m.predict(dte, iteration_range=(0, m.best_iteration+1))\n",
    "        oof_x[va_idx] = pv.astype(np.float32); pr_x += pt.astype(np.float32)/SKF.n_splits\n",
    "        print(f\"Fold {f}  XGB SMAPE={smape(inv(yva), inv(pv)):.3f}\")\n",
    "        del dtr, dva, dte, Xtr, Xva; gc.collect()\n",
    "    print(\"OOF  XGB :\", f\"{smape(y, inv(oof_x)):.3f}\")\n",
    "    oofs.append(oof_x); preds.append(pr_x); names.append(\"XGB\")\n",
    "\n",
    "print(\"\\nModels available:\", names)\n",
    "toc(t0, \"Models (Linear + LGB-GBDT + LGB-DART + XGB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:59:24.857273Z",
     "iopub.status.busy": "2025-10-12T14:59:24.856424Z",
     "iopub.status.idle": "2025-10-12T14:59:25.927642Z",
     "shell.execute_reply": "2025-10-12T14:59:25.926901Z",
     "shell.execute_reply.started": "2025-10-12T14:59:24.857249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Meta-learner stacker\n",
      "Stacker OOF SMAPE=48.961  weights=[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  sum=1.0000\n",
      "[END] Meta-learner stacker -> 0.90s\n",
      "\n",
      "[START] Residual bias corrections (log-space)\n",
      "[END] Residual bias corrections -> 0.16s\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Meta-learner stacker (simplex, non-negative) ----------------\n",
    "t0 = tic(\"Meta-learner stacker\")\n",
    "oof_stack  = np.stack(oofs,  axis=1).astype(np.float32)\n",
    "test_stack = np.stack(preds, axis=1).astype(np.float32)\n",
    "K = oof_stack.shape[1]\n",
    "\n",
    "# init by projected gradient on y_log (convex surrogate)\n",
    "w = np.ones(K, dtype=np.float32)/K\n",
    "for it in range(300):\n",
    "    grad = (oof_stack @ w - y_log).astype(np.float64) @ oof_stack\n",
    "    w = w - 0.2 * grad.astype(np.float32) / max(1.0, np.linalg.norm(grad))\n",
    "    w = np.maximum(w, 0.0); s = w.sum(); w = w/s if s>0 else np.ones(K, np.float32)/K\n",
    "\n",
    "def smape_score_w(wvec): return smape(y, inv(oof_stack @ wvec))\n",
    "\n",
    "best = smape_score_w(w.copy())\n",
    "for _ in range(4):\n",
    "    for k in range(K):\n",
    "        base = w.copy(); others = 1.0 - base[k]\n",
    "        span = np.linspace(max(0.0, base[k]-0.3), min(1.0, base[k]+0.3), 16, dtype=np.float32)\n",
    "        best_local, best_s = base.copy(), best\n",
    "        for ak in span:\n",
    "            wk = base.copy(); wk[k] = ak\n",
    "            if K > 1:\n",
    "                if others > 0:\n",
    "                    scale = (1.0 - ak)/others\n",
    "                    for j in range(K):\n",
    "                        if j != k: wk[j] = max(0.0, wk[j]*scale)\n",
    "                else:\n",
    "                    wk[:] = 0.0; wk[k] = 1.0\n",
    "            s = smape_score_w(wk)\n",
    "            if s < best_s: best_s, best_local = s, wk\n",
    "        w, best = best_local, best_s\n",
    "\n",
    "print(f\"Stacker OOF SMAPE={best:.3f}  weights={w.round(4).tolist()}  sum={float(w.sum()):.4f}\")\n",
    "final_log = test_stack @ w\n",
    "toc(t0, \"Meta-learner stacker\")\n",
    "\n",
    "# ---------------- Residual bias corrections (brand + unit + brand×unit) ----------------\n",
    "t0 = tic(\"Residual bias corrections (log-space)\")\n",
    "df_oof = pd.DataFrame({\n",
    "    \"brand\": train[\"brand_heur\"].fillna(\"\"),\n",
    "    \"unit\":  train[\"unit_extracted\"].fillna(\"\"),\n",
    "    \"bu\":    (train[\"brand_heur\"].fillna(\"\") + \"§\" + train[\"unit_extracted\"].fillna(\"\")).astype(str),\n",
    "    \"y\": y_log,\n",
    "    \"p\": (oof_stack @ w)\n",
    "})\n",
    "df_oof[\"res\"] = df_oof[\"y\"] - df_oof[\"p\"]\n",
    "\n",
    "def smooth_bias(key):\n",
    "    g = df_oof.groupby(key)[\"res\"].agg([\"mean\",\"count\"]).reset_index()\n",
    "    ALPHA = 14.0  # shrinkage\n",
    "    g[\"mean_smooth\"] = (g[\"mean\"] * g[\"count\"]) / (g[\"count\"] + ALPHA)\n",
    "    return dict(zip(g[key], g[\"mean_smooth\"]))\n",
    "\n",
    "bias_brand = smooth_bias(\"brand\")\n",
    "bias_unit  = smooth_bias(\"unit\")\n",
    "bias_bu    = smooth_bias(\"bu\")\n",
    "\n",
    "b_brand = test[\"brand_heur\"].fillna(\"\").map(bias_brand).fillna(0.0).values.astype(np.float32)\n",
    "b_unit  = test[\"unit_extracted\"].fillna(\"\").map(bias_unit).fillna(0.0).values.astype(np.float32)\n",
    "b_bu    = (test[\"brand_heur\"].fillna(\"\") + \"§\" + test[\"unit_extracted\"].fillna(\"\")).map(bias_bu).fillna(0.0).values.astype(np.float32)\n",
    "\n",
    "# Combine small biases; use conservative weights to avoid overfit\n",
    "final_log_bias = final_log + 0.7*b_brand + 0.4*b_unit + 0.5*b_bu\n",
    "toc(t0, \"Residual bias corrections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:59:35.420972Z",
     "iopub.status.busy": "2025-10-12T14:59:35.420400Z",
     "iopub.status.idle": "2025-10-12T14:59:35.577473Z",
     "shell.execute_reply": "2025-10-12T14:59:35.576875Z",
     "shell.execute_reply.started": "2025-10-12T14:59:35.420949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Post-process & submission\n",
      "\n",
      "Saved predictions to: /kaggle/working/test_out.csv\n",
      "   sample_id      price\n",
      "0     100179  13.531790\n",
      "1     245611  22.735712\n",
      "2     146263  17.402027\n",
      "3      95658   6.051785\n",
      "4      36806  27.372946\n",
      "5     148239   5.574152\n",
      "6      92659  11.784738\n",
      "7       3780  15.106535\n",
      "[END] Post-process & submission -> 0.15s\n",
      "\n",
      "Total runtime: 15389.99s\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Post-process & Submission ----------------\n",
    "t0 = tic(\"Post-process & submission\")\n",
    "preds = inv(final_log_bias)\n",
    "q1, q3 = np.percentile(y, [25, 75]); iqr = q3 - q1; hi = q3 + 3.0*iqr\n",
    "preds = np.clip(preds, 0.01, hi*5).astype(np.float64)\n",
    "\n",
    "sub = pd.DataFrame({\"sample_id\": test[\"sample_id\"].astype(int).values, \"price\": preds})[[\"sample_id\",\"price\"]]\n",
    "OUT_PATH = WORK_DIR / \"test_out.csv\"\n",
    "sub.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\nSaved predictions to: {OUT_PATH.resolve()}\")\n",
    "print(sub.head(8))\n",
    "toc(t0, \"Post-process & submission\")\n",
    "\n",
    "print(f\"\\nTotal runtime: {time.time()-T0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T21:05:05.272861Z",
     "iopub.status.busy": "2025-10-11T21:05:05.272198Z",
     "iopub.status.idle": "2025-10-11T21:05:06.419396Z",
     "shell.execute_reply": "2025-10-11T21:05:06.418545Z",
     "shell.execute_reply.started": "2025-10-11T21:05:05.272837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8454579,
     "sourceId": 13334227,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
